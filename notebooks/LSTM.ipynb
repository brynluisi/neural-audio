{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Audio LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd328224f10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.nn import Module\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio.functional import lfilter\n",
    "\n",
    "# For Reproducibility\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET = \"ht1\"  # change to ht1 or muff\n",
    "TRAIN = True  # change to False to skip training\n",
    "VERSION = \"v1\"  # increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralAudioDataSet(Dataset):\n",
    "    \"\"\"\n",
    "    Creates dataset object for training, evaluation, and prediction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, target, sequence_length):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "\n",
    "        self._sequence_length = sequence_length\n",
    "        self.input_sequence = self.wrap_to_sequences(self.input, self._sequence_length)\n",
    "        self.target_sequence = self.wrap_to_sequences(\n",
    "            self.target, self._sequence_length\n",
    "        )\n",
    "        self._len = self.input_sequence.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"input\": self.input_sequence[index, :, :],\n",
    "            \"target\": self.target_sequence[index, :, :],\n",
    "        }\n",
    "\n",
    "    def wrap_to_sequences(self, data, sequence_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: Either input or target signal\n",
    "            sequence_length: Number of samples in training example\n",
    "\n",
    "        Returns:\n",
    "            wrapped_data: Data packed into a sequence tensor for LSTM\n",
    "        \"\"\"\n",
    "        num_sequences = int(np.floor(data.shape[0] / sequence_length))\n",
    "        truncated_data = data[0 : (num_sequences * sequence_length)]\n",
    "        wrapped_data = truncated_data.reshape((num_sequences, sequence_length, 1))\n",
    "        wrapped_data = wrapped_data.permute(0, 2, 1)\n",
    "\n",
    "        return np.float32(wrapped_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class IIRNN(Module):\n",
    "    \"\"\"\n",
    "    Defines an LSTM model for guitar distortion modeling\n",
    "\n",
    "    Attributes:\n",
    "        input_size: Number of channels in input signal\n",
    "        output_size: Number of channels to predict for output signal\n",
    "        hidden_size: Number of features for a single audio sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=80):\n",
    "        super(IIRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_size, hidden_size=self.hidden_size, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x: Input signal from dataloader, [batch_size, input_size, sequence_length]\n",
    "\n",
    "        Returns:\n",
    "            x: Output signal, [batch_size, input_size, sequence_length]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        x, hn = self.lstm(x.permute(0, 2, 1))\n",
    "\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(criterion, model, loader, optimizer, is_train):\n",
    "\n",
    "    if is_train:\n",
    "        model.train(True)\n",
    "    else:\n",
    "        model.train(False)\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    total_loss = 0\n",
    "\n",
    "    for ind, batch in enumerate(loader):\n",
    "        input_seq_batch = batch[\"input\"].to(device)\n",
    "        target_seq_batch = batch[\"target\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_output = model(input_seq_batch)\n",
    "\n",
    "        # Apply pre-emphasis filter to minimize loss in important frequency range\n",
    "        target_seq_batch_filt = lfilter(\n",
    "            target_seq_batch, torch.Tensor([1, 0]), torch.Tensor([1, -0.95])\n",
    "        )\n",
    "        predicted_output_filt = lfilter(\n",
    "            predicted_output, torch.Tensor([1, 0]), torch.Tensor([1, -0.95])\n",
    "        )\n",
    "\n",
    "        loss = criterion(target_seq_batch_filt, predicted_output_filt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # print(f\"Loss: {loss}\")\n",
    "\n",
    "    total_loss /= len(loader)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def inspect_file(path):\n",
    "    print(\"-\" * 10)\n",
    "    print(\"Source:\", path)\n",
    "    print(\"-\" * 10)\n",
    "    print(f\" - File size: {os.path.getsize(path)} bytes\")\n",
    "    print(f\" - {torchaudio.info(path)}\")\n",
    "\n",
    "\n",
    "# Save audio files\n",
    "def save_audio(batch):\n",
    "    out_batch = batch.detach().cpu()\n",
    "    out_batch = out_batch.squeeze(-1).flatten()\n",
    "    print(out_batch.shape)\n",
    "    return out_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device= cpu\n"
     ]
    }
   ],
   "source": [
    "dirname = os.path.abspath(\"\")\n",
    "rootdir = os.path.split(dirname)[0]\n",
    "\n",
    "TRAINING_INPUT_PATH = \"\".join([rootdir, f\"/data/train/{DATASET}-input.wav\"])\n",
    "TRAINING_TARGET_PATH = \"\".join([rootdir, f\"/data/train/{DATASET}-target.wav\"])\n",
    "VAL_INPUT_PATH = \"\".join([rootdir, f\"/data/val/{DATASET}-input.wav\"])\n",
    "VAL_TARGET_PATH = \"\".join([rootdir, f\"/data/val/{DATASET}-input.wav\"])\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device=\", device)\n",
    "train_input, input_fs = torchaudio.load(TRAINING_INPUT_PATH)\n",
    "train_target, target_fs = torchaudio.load(TRAINING_TARGET_PATH)\n",
    "\n",
    "val_input, _ = torchaudio.load(VAL_INPUT_PATH)\n",
    "val_target, _ = torchaudio.load(VAL_TARGET_PATH)\n",
    "\n",
    "assert input_fs == target_fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "\n",
      "LOSS train 0.004995518347381481 valid 3.2538740924792364e-05\n",
      "LOSS train 0.0049940382263490134 valid 3.255289630033076e-05\n",
      "LOSS train 0.004992566919619483 valid 3.0310255169752054e-05\n",
      "LOSS train 0.004981764825060964 valid 2.5153863134619314e-05\n",
      "LOSS train 0.00498095472409789 valid 2.5209339582943358e-05\n"
     ]
    }
   ],
   "source": [
    "# Define dataloader for training data\n",
    "batch_size = 1024\n",
    "sequence_length = 1024\n",
    "train_dataset = NeuralAudioDataSet(\n",
    "    train_input.squeeze(0), train_target.squeeze(0), sequence_length\n",
    ")\n",
    "loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_dataset = NeuralAudioDataSet(\n",
    "    val_input.squeeze(0), val_target.squeeze(0), sequence_length\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "if TRAIN:\n",
    "    # Define optimizer, model, and criterion for training\n",
    "    model = IIRNN()\n",
    "    n_epochs = 100\n",
    "    lr = 1e-3\n",
    "\n",
    "    optimizer = Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-08,\n",
    "        weight_decay=0,\n",
    "        amsgrad=False,\n",
    "    )\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Run training loop\n",
    "    print(\"Training started\\n\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train(criterion, model, loader, optimizer, True)\n",
    "        val_loss = train(criterion, model, val_loader, optimizer, False)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print('LOSS train {} valid {}'.format(train_loss, val_loss))\n",
    "\n",
    "    # Save model\n",
    "    save_path = os.path.join(\n",
    "        f\"../models/lstm-{DATASET}-{VERSION}\"\n",
    "    )\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "else:\n",
    "    load_path = os.path.join(\n",
    "        f\"../models/lstm-{DATASET}-{VERSION}\"\n",
    "    )\n",
    "\n",
    "    model = IIRNN()\n",
    "    model.load_state_dict(torch.load(load_path))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Apply and save model, write audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write audio output\n",
    "out_path = \"../output/audio\"\n",
    "sample_rate = 44100\n",
    "save_tensor = torch.zeros(train_input.shape[1] // sequence_length, 1024)\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(loader):\n",
    "        input_seq_batch = batch[\"input\"].to(device)\n",
    "        predicted_output = model(input_seq_batch)\n",
    "        output_tmp = predicted_output.squeeze().detach().cpu()\n",
    "        save_tensor[i, :] = output_tmp\n",
    "\n",
    "    out_audio = save_audio(save_tensor.view(-1,1))\n",
    "    path = os.path.join(out_path, f\"lstm-{DATASET}-{VERSION}.wav\")\n",
    "    print(\"Exporting {}\".format(path))\n",
    "    sf.write(path, out_audio, sample_rate,'PCM_24')\n",
    "    # out_audio = out_audio.view(1,-1)\n",
    "    # torchaudio.save(\n",
    "    #     path, out_audio, sample_rate, encoding=\"PCM_S\", bits_per_sample=16\n",
    "    # )\n",
    "    inspect_file(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3/anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
