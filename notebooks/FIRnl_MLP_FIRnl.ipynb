{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Module, Parameter\n",
    "from torch import FloatTensor\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "from torchaudio import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device= cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device=\", device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44100\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = './data/train'\n",
    "val_data_dir = './data/val'\n",
    "input_name = 'ht1-input.wav'\n",
    "target_name = 'ht1-target.wav'\n",
    "train_sig, _ = torchaudio.load(os.path.join(train_data_dir, input_name))\n",
    "val_sig, sr = torchaudio.load(os.path.join(val_data_dir, input_name))\n",
    "train_target_sig, _ = torchaudio.load(os.path.join(train_data_dir, target_name))\n",
    "val_target_sig, _ = torchaudio.load(os.path.join(val_data_dir, target_name))\n",
    "print(sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we need transform here? maybe not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # transform : downsample here \n",
    "# new_sr = 8000 #10000\n",
    "# transform = transforms.Resample(orig_freq=sr, new_freq=new_sr)\n",
    "# train_input = transform(sig)\n",
    "# train_target = transform(tar_sig)\n",
    "# ipd.Audio(train_input.numpy(), rate=new_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_sig[:,:44100*60]\n",
    "train_target = train_target_sig[:,:44100*60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b9b006ad290>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIIRDataSet(Dataset):\n",
    "    def __init__(self, input, target, sequence_length):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self._sequence_length = sequence_length\n",
    "        self.input_sequence = self.wrap_to_sequences(self.input, self._sequence_length)\n",
    "        self.target_sequence = self.wrap_to_sequences(self.target, self._sequence_length)\n",
    "        self._len = self.input_sequence.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'input': self.input_sequence[index, :, :]\n",
    "               ,'target': self.target_sequence[index, :, :]}\n",
    "\n",
    "    def wrap_to_sequences(self, data, sequence_length):\n",
    "        num_sequences = int(np.floor(data.shape[0] / sequence_length))\n",
    "        print(num_sequences)\n",
    "        truncated_data = data[0:(num_sequences * sequence_length)]\n",
    "        wrapped_data = truncated_data.reshape((num_sequences, sequence_length, 1))\n",
    "        wrapped_data = wrapped_data.permute(0,2,1)\n",
    "        print(wrapped_data.shape)\n",
    "        return np.float32(wrapped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2646000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5167\n",
      "torch.Size([5167, 1, 512])\n",
      "5167\n",
      "torch.Size([5167, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512#1024\n",
    "sequence_length = 512\n",
    "train_dataset=DIIRDataSet(train_input.squeeze(0), train_target.squeeze(0), sequence_length)\n",
    "loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = False, pin_memory=True, drop_last=True) #? what does the shuffle really shuffles here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIRNN(Module):\n",
    "    def __init__(self, n_input=1, n_output=1, kernel_size=80, n_channel=32):\n",
    "        super(FIRNN, self).__init__()\n",
    "        self.conv_kz = kernel_size\n",
    "        self.input_len = 512\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=kernel_size, stride=1)\n",
    "        self.nonlinear = nn.Tanh()\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "\n",
    "        self.fc1 = nn.Conv1d(n_channel, n_channel*2, kernel_size=1) \n",
    "        self.fc2 = nn.Conv1d(n_channel*2, n_channel, kernel_size=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(n_channel, n_output, kernel_size=kernel_size, stride=1)\n",
    "        \n",
    "        self.mlp_layer = nn.Sequential(\n",
    "            self.fc1 ,\n",
    "            nn.Tanh(),\n",
    "            self.fc2,\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        bs = x.shape[0]\n",
    "    \n",
    "        # first get fir output\n",
    "        x = F.pad(x, (self.conv_kz-1, 0)) #pad on the left side\n",
    "        x = self.conv1(x) \n",
    "        #print(x.shape)\n",
    "        x = self.nonlinear(self.bn1(x))\n",
    "        #print(x.shape)\n",
    "\n",
    "        #x = x.view(bs, -1)\n",
    "        x = self.mlp_layer(x)\n",
    "        #x = self.layers(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = F.pad(x, (self.conv_kz-1, 0))\n",
    "        x = self.nonlinear(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FIRNN(kernel_size=80, n_channel=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimizer and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "n_epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(criterion, model, loader, optimizer):\n",
    "    model.train()\n",
    "    device = next(model.parameters()).device\n",
    "    total_loss = 0\n",
    "    \n",
    "    for ind, batch in enumerate(loader):\n",
    "        input_seq_batch = batch['input'].to(device)\n",
    "        target_seq_batch = batch['target'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predicted_output = model(input_seq_batch)\n",
    "        loss = criterion(target_seq_batch, predicted_output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    total_loss /= len(loader)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -- Loss 7.262408E-02\n",
      "Epoch 1 -- Loss 6.325756E-02\n",
      "Epoch 2 -- Loss 6.612604E-02\n",
      "Epoch 3 -- Loss 6.803681E-02\n",
      "Epoch 4 -- Loss 6.776053E-02\n",
      "Epoch 5 -- Loss 6.346564E-02\n",
      "Epoch 6 -- Loss 6.065446E-02\n",
      "Epoch 7 -- Loss 6.015028E-02\n",
      "Epoch 8 -- Loss 6.155975E-02\n",
      "Epoch 9 -- Loss 6.044480E-02\n",
      "Epoch 10 -- Loss 5.822162E-02\n",
      "Epoch 11 -- Loss 5.794315E-02\n",
      "Epoch 12 -- Loss 5.957253E-02\n",
      "Epoch 13 -- Loss 6.114915E-02\n",
      "Epoch 14 -- Loss 6.039316E-02\n",
      "Epoch 15 -- Loss 5.939793E-02\n",
      "Epoch 16 -- Loss 5.935268E-02\n",
      "Epoch 17 -- Loss 5.711304E-02\n",
      "Epoch 18 -- Loss 5.657114E-02\n",
      "Epoch 19 -- Loss 5.626238E-02\n",
      "Epoch 20 -- Loss 5.628797E-02\n",
      "Epoch 21 -- Loss 5.826691E-02\n",
      "Epoch 22 -- Loss 6.059146E-02\n",
      "Epoch 23 -- Loss 5.993516E-02\n",
      "Epoch 24 -- Loss 6.206088E-02\n",
      "Epoch 25 -- Loss 6.466783E-02\n",
      "Epoch 26 -- Loss 6.528736E-02\n",
      "Epoch 27 -- Loss 6.446791E-02\n",
      "Epoch 28 -- Loss 6.423380E-02\n",
      "Epoch 29 -- Loss 6.200980E-02\n",
      "Epoch 30 -- Loss 5.940140E-02\n",
      "Epoch 31 -- Loss 5.847254E-02\n",
      "Epoch 32 -- Loss 5.532814E-02\n",
      "Epoch 33 -- Loss 5.618483E-02\n",
      "Epoch 34 -- Loss 5.453051E-02\n",
      "Epoch 35 -- Loss 5.440785E-02\n",
      "Epoch 36 -- Loss 5.335998E-02\n",
      "Epoch 37 -- Loss 5.345599E-02\n",
      "Epoch 38 -- Loss 5.399338E-02\n",
      "Epoch 39 -- Loss 5.392373E-02\n",
      "Epoch 40 -- Loss 5.540484E-02\n",
      "Epoch 41 -- Loss 5.572603E-02\n",
      "Epoch 42 -- Loss 5.603988E-02\n",
      "Epoch 43 -- Loss 5.641749E-02\n",
      "Epoch 44 -- Loss 5.458917E-02\n",
      "Epoch 45 -- Loss 5.351142E-02\n",
      "Epoch 46 -- Loss 5.323004E-02\n",
      "Epoch 47 -- Loss 5.278331E-02\n",
      "Epoch 48 -- Loss 5.240671E-02\n",
      "Epoch 49 -- Loss 5.141523E-02\n",
      "Epoch 50 -- Loss 5.138933E-02\n",
      "Epoch 51 -- Loss 5.200745E-02\n",
      "Epoch 52 -- Loss 5.198990E-02\n",
      "Epoch 53 -- Loss 5.253568E-02\n",
      "Epoch 54 -- Loss 5.233016E-02\n",
      "Epoch 55 -- Loss 5.562179E-02\n",
      "Epoch 56 -- Loss 5.710824E-02\n",
      "Epoch 57 -- Loss 5.504415E-02\n",
      "Epoch 58 -- Loss 5.259318E-02\n",
      "Epoch 59 -- Loss 5.128343E-02\n",
      "Epoch 60 -- Loss 5.047180E-02\n",
      "Epoch 61 -- Loss 5.245109E-02\n",
      "Epoch 62 -- Loss 5.401845E-02\n",
      "Epoch 63 -- Loss 5.602268E-02\n",
      "Epoch 64 -- Loss 5.817734E-02\n",
      "Epoch 65 -- Loss 5.817537E-02\n",
      "Epoch 66 -- Loss 5.519110E-02\n",
      "Epoch 67 -- Loss 5.352652E-02\n",
      "Epoch 68 -- Loss 5.074314E-02\n",
      "Epoch 69 -- Loss 5.026576E-02\n",
      "Epoch 70 -- Loss 5.114615E-02\n",
      "Epoch 71 -- Loss 5.046448E-02\n",
      "Epoch 72 -- Loss 4.979381E-02\n",
      "Epoch 73 -- Loss 4.875106E-02\n",
      "Epoch 74 -- Loss 4.854107E-02\n",
      "Epoch 75 -- Loss 4.929529E-02\n",
      "Epoch 76 -- Loss 4.930580E-02\n",
      "Epoch 77 -- Loss 4.904496E-02\n",
      "Epoch 78 -- Loss 4.847446E-02\n",
      "Epoch 79 -- Loss 4.815025E-02\n",
      "Epoch 80 -- Loss 4.865781E-02\n",
      "Epoch 81 -- Loss 4.867778E-02\n",
      "Epoch 82 -- Loss 4.863456E-02\n",
      "Epoch 83 -- Loss 4.833643E-02\n",
      "Epoch 84 -- Loss 4.821110E-02\n",
      "Epoch 85 -- Loss 4.871698E-02\n",
      "Epoch 86 -- Loss 4.895082E-02\n",
      "Epoch 87 -- Loss 4.890053E-02\n",
      "Epoch 88 -- Loss 4.853452E-02\n",
      "Epoch 89 -- Loss 4.905550E-02\n",
      "Epoch 90 -- Loss 4.848647E-02\n",
      "Epoch 91 -- Loss 4.774929E-02\n",
      "Epoch 92 -- Loss 4.908007E-02\n",
      "Epoch 93 -- Loss 5.081439E-02\n",
      "Epoch 94 -- Loss 5.183116E-02\n",
      "Epoch 95 -- Loss 5.423207E-02\n",
      "Epoch 96 -- Loss 5.526942E-02\n",
      "Epoch 97 -- Loss 5.210524E-02\n",
      "Epoch 98 -- Loss 5.067018E-02\n",
      "Epoch 99 -- Loss 4.984730E-02\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    loss = train(criterion, model, loader, optimizer)\n",
    "    print(\"Epoch {} -- Loss {:3E}\".format(epoch, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('./models/model_WienerHammersteinNonlin_k80_bs512_ch32_44khz_ep{}.pth'.format(n_epochs-1))\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5167\n",
      "torch.Size([5167, 1, 512])\n",
      "5167\n",
      "torch.Size([5167, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "val_batch_size = 128\n",
    "sequence_length = 512\n",
    "val_dataset=DIIRDataSet(train_input.squeeze(0), train_target.squeeze(0), sequence_length)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle = False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_file(path):\n",
    "    print(\"-\" * 10)\n",
    "    print(\"Source:\", path)\n",
    "    print(\"-\" * 10)\n",
    "    print(f\" - File size: {os.path.getsize(path)} bytes\")\n",
    "    print(f\" - {torchaudio.info(path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_audio(batch):\n",
    "    #1024,512,1\n",
    "    out_batch = batch.detach().cpu()\n",
    "    out_batch = out_batch.squeeze(-1).flatten()\n",
    "    print(out_batch.shape)\n",
    "    return out_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5167, 512])\n",
      "torch.Size([2645504])\n",
      "torch.Size([2645504])\n",
      "Exporting ./output/target_WienerHammersteinNonlin_bs512_ch32_44khz.wav\n",
      "----------\n",
      "Source: ./output/target_WienerHammersteinNonlin_bs512_ch32_44khz.wav\n",
      "----------\n",
      " - File size: 7936556 bytes\n",
      " - AudioMetaData(sample_rate=44100, num_frames=2645504, num_channels=1, bits_per_sample=24, encoding=PCM_S)\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "out_path = './output/'\n",
    "sample_rate = 44100\n",
    "save_tensor = torch.zeros(5167,512)\n",
    "with torch.no_grad():\n",
    "    for i, val_batch in enumerate(val_loader):\n",
    "        input_seq_batch = val_batch['input'].to(device)\n",
    "        #target_seq_batch = val_batch['target'].to(device)\n",
    "        predicted_output = model(input_seq_batch)\n",
    "        output_tmp = predicted_output.squeeze().detach().cpu()\n",
    "        #print(output_tmp.shape)\n",
    "        save_tensor[i,:] = output_tmp\n",
    "    \n",
    "    print(save_tensor.shape)\n",
    "    out_audio = save_audio(save_tensor)\n",
    "    print(out_audio.shape)\n",
    "    path = os.path.join(out_path, \"target_WienerHammersteinNonlin_bs512_ch32_44khz.wav\")\n",
    "    print(\"Exporting {}\".format(path))\n",
    "    sf.write(path, out_audio, sample_rate,'PCM_24')\n",
    "    #torchaudio.save(path, out_audio, sample_rate, encoding=\"PCM_S\", bits_per_sample=16)\n",
    "    inspect_file(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4])\n",
      "torch.Size([3, 3, 7])\n"
     ]
    }
   ],
   "source": [
    "# #small test on padding\n",
    "# t4d = torch.ones(3, 3, 4)\n",
    "# print(t4d.shape)\n",
    "# out = F.pad(t4d, (3,0)) #\"constant\", 0\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4d[1,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1,1,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3/anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
