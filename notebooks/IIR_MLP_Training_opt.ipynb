{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioMetaData(sample_rate=44100, num_frames=14994001, num_channels=1, bits_per_sample=16, encoding=PCM_S)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Module, Parameter\n",
    "from torch import FloatTensor\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "from torchaudio import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.signal import sosfiltfilt\n",
    "import os\n",
    "dirname = os.path.abspath('')\n",
    "rootdir = os.path.split(dirname)[0]\n",
    "\n",
    "H1_TRAINING_INPUT_PATH = \"\".join([rootdir, \"/data/train/ht1-input.wav\"])\n",
    "H1_TRAINING_TARGET_PATH = \"\".join([rootdir, \"/data/train/ht1-target.wav\"])\n",
    "\n",
    "MUFF_TRAINING_INPUT_PATH = \"\".join([rootdir, \"/data/train/muff-input.wav\"])\n",
    "MUFF_TRAINING_TARGET_PATH = \"\".join([rootdir, \"/data/train/muff-target.wav\"])\n",
    "\n",
    "metadata = torchaudio.info(H1_TRAINING_INPUT_PATH)\n",
    "print(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device= cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device=\", device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, fs = torchaudio.load(MUFF_TRAINING_INPUT_PATH)\n",
    "train_target, fs = torchaudio.load(MUFF_TRAINING_TARGET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14994001])\n",
      "torch.Size([1, 14994001])\n"
     ]
    }
   ],
   "source": [
    "#len(train_input)\n",
    "print(train_input.shape)\n",
    "print(train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_array = train_input.numpy().reshape(-1)\n",
    "train_target_array = train_target.numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff510667290>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAudioDataSet(Dataset):\n",
    "    def __init__(self, input, target, sequence_length):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        \n",
    "        self._sequence_length = sequence_length\n",
    "        self.input_sequence = self.wrap_to_sequences(self.input, self._sequence_length)\n",
    "        self.target_sequence = self.wrap_to_sequences(self.target, self._sequence_length)\n",
    "        self._len = self.input_sequence.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'input': self.input_sequence[index, :, :]\n",
    "               ,'target': self.target_sequence[index, :, :]}\n",
    "\n",
    "    def wrap_to_sequences(self, data, sequence_length):\n",
    "        num_sequences = int(np.floor(data.shape[0] / sequence_length))\n",
    "        print(num_sequences)\n",
    "        truncated_data = data[0:(num_sequences * sequence_length)]\n",
    "        wrapped_data = truncated_data.reshape((num_sequences, sequence_length, 1))\n",
    "        wrapped_data = wrapped_data.permute(0,2,1)\n",
    "        print(wrapped_data.shape)\n",
    "        return np.float32(wrapped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14994001])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14642\n",
      "torch.Size([14642, 1, 1024])\n",
      "14642\n",
      "torch.Size([14642, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 #1024\n",
    "sequence_length = 1024\n",
    "train_dataset=NeuralAudioDataSet(train_input.squeeze(0), train_target.squeeze(0), sequence_length)\n",
    "loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = False, pin_memory=True, drop_last=True) #? what does the shuffle really shuffles here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IIRNN(Module):\n",
    "    def __init__(self, n_input=1, n_output=1, hidden_size=80, n_channel=1):\n",
    "        super(IIRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # \n",
    "        self.lstm = nn.LSTM(input_size = 1, hidden_size = self.hidden_size, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "        self.mlp_layer = nn.Sequential(\n",
    "            self.fc1 ,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # sequence_length: nsamples in 1 training example\n",
    "        # batch_size: number of groups of audio samples\n",
    "        # input_size: nchannels for each audio sample: 1\n",
    "        # hidden_size: number of features for a single audio sample\n",
    "        \n",
    "        x, hn = self.lstm(x.permute(0,2,1)) # output; (sequence_length, batch_size, hidden_size)\n",
    "        \n",
    "#         print(x.shape)\n",
    "        \n",
    "        x = self.mlp_layer(x)\n",
    "\n",
    "        return x.permute(0,2,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IIRNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimizer and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from ignite.metrics import PSNR\n",
    "\n",
    "n_epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "def esr(output, target):\n",
    "    loss = torch.mean(torch.abs(target - output)**2) / (torch.mean(torch.abs(target)**2) + 1e-6)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def edc(output, target):\n",
    "    \n",
    "    step_one = torch.abs(torch.mean(target - output, axis = 2))**2\n",
    "    \n",
    "    # avg over time steps but not batch here\n",
    "    \n",
    "    step_two = torch.mean(step_one)\n",
    "    \n",
    "    step_three = step_two / (torch.mean(torch.abs(target)**2) + 1e-6)\n",
    "    \n",
    "    return step_three\n",
    "\n",
    "def etotal(output, target): \n",
    "    return esr(output, target) + edc(output, target)\n",
    "    \n",
    "\n",
    "# criterion = etotal()\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from torchaudio.functional import lfilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(criterion, model, loader, optimizer):\n",
    "    model.train()\n",
    "    device = next(model.parameters()).device\n",
    "    total_loss = 0\n",
    "    \n",
    "    for ind, batch in enumerate(loader):\n",
    "        input_seq_batch = batch['input'].to(device)\n",
    "        target_seq_batch = batch['target'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_output = model(input_seq_batch)\n",
    "        \n",
    "        \n",
    "        target_seq_batch_filt = lfilter(target_seq_batch, torch.Tensor([1,0]), torch.Tensor([1, -0.95]))\n",
    "        predicted_output_filt = lfilter(predicted_output, torch.Tensor([1,0]), torch.Tensor([1, -0.95]))\n",
    "        \n",
    "        loss = criterion(target_seq_batch_filt, predicted_output_filt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "#         print(f\"Loss: {loss}\")\n",
    "        \n",
    "\n",
    "    total_loss /= len(loader)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -- Loss 3.911578E-04\n",
      "Epoch 1 -- Loss 3.826969E-04\n",
      "Epoch 2 -- Loss 3.550321E-04\n",
      "Epoch 3 -- Loss 3.368950E-04\n",
      "Epoch 4 -- Loss 3.026051E-04\n",
      "Epoch 5 -- Loss 3.200919E-04\n",
      "Epoch 6 -- Loss 2.931160E-04\n",
      "Epoch 7 -- Loss 2.710257E-04\n",
      "Epoch 8 -- Loss 2.630627E-04\n",
      "Epoch 9 -- Loss 2.609727E-04\n",
      "Epoch 10 -- Loss 2.608075E-04\n",
      "Epoch 11 -- Loss 2.864918E-04\n",
      "Epoch 12 -- Loss 2.799091E-04\n",
      "Epoch 13 -- Loss 2.702745E-04\n",
      "Epoch 14 -- Loss 2.555225E-04\n",
      "Epoch 15 -- Loss 2.689694E-04\n",
      "Epoch 16 -- Loss 2.528190E-04\n",
      "Epoch 17 -- Loss 2.548809E-04\n",
      "Epoch 18 -- Loss 2.546860E-04\n",
      "Epoch 19 -- Loss 2.523009E-04\n",
      "Epoch 20 -- Loss 2.519202E-04\n",
      "Epoch 21 -- Loss 2.507434E-04\n",
      "Epoch 22 -- Loss 2.502143E-04\n",
      "Epoch 23 -- Loss 2.493604E-04\n",
      "Epoch 24 -- Loss 2.486217E-04\n",
      "Epoch 25 -- Loss 2.474392E-04\n",
      "Epoch 26 -- Loss 2.468442E-04\n",
      "Epoch 27 -- Loss 2.470597E-04\n",
      "Epoch 28 -- Loss 2.462033E-04\n",
      "Epoch 29 -- Loss 2.453266E-04\n",
      "Epoch 30 -- Loss 2.414719E-04\n",
      "Epoch 31 -- Loss 2.433626E-04\n",
      "Epoch 32 -- Loss 2.404930E-04\n",
      "Epoch 33 -- Loss 2.390721E-04\n",
      "Epoch 34 -- Loss 2.388078E-04\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    loss = train(criterion, model, loader, optimizer)\n",
    "    print(\"Epoch {} -- Loss {:3E}\".format(epoch, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('../models/lstm_mlp_premp_filter_mse_muff'.format(n_epochs-1))\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch_size = 128\n",
    "sequence_length = 80\n",
    "val_dataset=DIIRDataSet(train_input.squeeze(0), train_target.squeeze(0), sequence_length)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle = False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_file(path):\n",
    "    print(\"-\" * 10)\n",
    "    print(\"Source:\", path)\n",
    "    print(\"-\" * 10)\n",
    "    print(f\" - File size: {os.path.getsize(path)} bytes\")\n",
    "    print(f\" - {torchaudio.info(path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_audio(batch):\n",
    "    #1024,512,1\n",
    "    out_batch = batch.detach().cpu()\n",
    "    out_batch = out_batch.squeeze(-1).flatten()\n",
    "    print(out_batch.shape)\n",
    "    return out_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "out_path = '../output/'\n",
    "sample_rate = 44100\n",
    "save_tensor = torch.zeros(14994001,80)\n",
    "with torch.no_grad():\n",
    "    for i, val_batch in enumerate(val_loader):\n",
    "        input_seq_batch = val_batch['input'].to(device)\n",
    "        #target_seq_batch = val_batch['target'].to(device)\n",
    "        predicted_output = model(input_seq_batch)\n",
    "        output_tmp = predicted_output.squeeze().detach().cpu()\n",
    "        #print(output_tmp.shape)\n",
    "        save_tensor[i,:] = output_tmp\n",
    "    \n",
    "    print(save_tensor.shape)\n",
    "    out_audio = save_audio(save_tensor)\n",
    "    print(out_audio.shape)\n",
    "    path = os.path.join(out_path, \"lstm_mlp_premp_filter_mse_muff.wav\")\n",
    "    print(\"Exporting {}\".format(path))\n",
    "    sf.write(path, out_audio, sample_rate,'PCM_24')\n",
    "    #torchaudio.save(path, out_audio, sample_rate, encoding=\"PCM_S\", bits_per_sample=16)\n",
    "    inspect_file(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
