{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioMetaData(sample_rate=44100, num_frames=14994001, num_channels=1, bits_per_sample=16, encoding=PCM_S)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Module, Parameter\n",
    "from torch import FloatTensor\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "from torchaudio import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.signal import sosfiltfilt\n",
    "import os\n",
    "dirname = os.path.abspath('')\n",
    "rootdir = os.path.split(dirname)[0]\n",
    "\n",
    "H1_TRAINING_INPUT_PATH = \"\".join([rootdir, \"/data/train/ht1-input.wav\"])\n",
    "H1_TRAINING_TARGET_PATH = \"\".join([rootdir, \"/data/train/ht1-target.wav\"])\n",
    "\n",
    "metadata = torchaudio.info(H1_TRAINING_INPUT_PATH)\n",
    "print(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device= cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device=\", device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, fs = torchaudio.load(H1_TRAINING_INPUT_PATH)\n",
    "train_target, fs = torchaudio.load(H1_TRAINING_TARGET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14994001])\n",
      "torch.Size([1, 14994001])\n"
     ]
    }
   ],
   "source": [
    "#len(train_input)\n",
    "print(train_input.shape)\n",
    "print(train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_array = train_input.numpy().reshape(-1)\n",
    "train_target_array = train_target.numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8cea174290>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIIRDataSet(Dataset):\n",
    "    def __init__(self, input, target, sequence_length):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        \n",
    "        self._sequence_length = sequence_length\n",
    "        self.input_sequence = self.wrap_to_sequences(self.input, self._sequence_length)\n",
    "        self.target_sequence = self.wrap_to_sequences(self.target, self._sequence_length)\n",
    "        self._len = self.input_sequence.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'input': self.input_sequence[index, :, :]\n",
    "               ,'target': self.target_sequence[index, :, :]}\n",
    "\n",
    "    def wrap_to_sequences(self, data, sequence_length):\n",
    "        num_sequences = int(np.floor(data.shape[0] / sequence_length))\n",
    "        print(num_sequences)\n",
    "        truncated_data = data[0:(num_sequences * sequence_length)]\n",
    "        wrapped_data = truncated_data.reshape((num_sequences, sequence_length, 1))\n",
    "        wrapped_data = wrapped_data.permute(0,2,1)\n",
    "        print(wrapped_data.shape)\n",
    "        return np.float32(wrapped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14994001])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187425\n",
      "torch.Size([187425, 1, 80])\n",
      "187425\n",
      "torch.Size([187425, 1, 80])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512#1024\n",
    "sequence_length = 80\n",
    "train_dataset=DIIRDataSet(train_input.squeeze(0), train_target.squeeze(0), sequence_length)\n",
    "loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = False, pin_memory=True, drop_last=True) #? what does the shuffle really shuffles here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IIRNN(Module):\n",
    "    def __init__(self, n_input=1, n_output=1, hidden_size=80, n_channel=1):\n",
    "        super(IIRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size = 80, hidden_size = self.hidden_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.hidden_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 80)\n",
    "#         self.fc3 = nn.Linear(80 , 80)\n",
    "\n",
    "        \n",
    "        self.mlp_layer = nn.Sequential(\n",
    "            self.fc1 ,\n",
    "            nn.Tanh(),\n",
    "            self.fc2,\n",
    "#             nn.Tanh(),\n",
    "#             self.fc3\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "    \n",
    "#         print(f\"xshape: {x.shape}\")\n",
    "        x, hn = self.lstm(x) # output; (sequence_length, batch_size, hidden_size)\n",
    "#         print(f\"x rnn shape: {x.shape}\")\n",
    "        x = self.mlp_layer(x)\n",
    "        x, hn = self.lstm(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IIRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimizer and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "n_epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 80])\n",
      "torch.Size([512, 1, 80])\n"
     ]
    }
   ],
   "source": [
    "for ind, batch in enumerate(loader):\n",
    "    input_seq_batch = batch['input'].to(device)\n",
    "    target_seq_batch = batch['target'].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    predicted_output = model(input_seq_batch)\n",
    "    print(input_seq_batch.shape)\n",
    "    print(predicted_output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(criterion, model, loader, optimizer):\n",
    "    model.train()\n",
    "    device = next(model.parameters()).device\n",
    "    total_loss = 0\n",
    "    \n",
    "    for ind, batch in enumerate(loader):\n",
    "        input_seq_batch = batch['input'].to(device)\n",
    "        target_seq_batch = batch['target'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predicted_output = model(input_seq_batch)\n",
    "\n",
    "        # premphasis filter\n",
    "        target_seq_batch_filt = signal.filtfilt([1, -0.95], [1], target_seq_batch)\n",
    "        predicted_output_filt = signal.filtfilt([1, -0.95], [1], predicted_output)\n",
    "        \n",
    "        loss = criterion(target_seq_batch_filt, predicted_output_filt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    total_loss /= len(loader)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -- Loss 5.729002E-02\n",
      "Epoch 1 -- Loss 3.587623E-02\n",
      "Epoch 2 -- Loss 3.248468E-02\n",
      "Epoch 3 -- Loss 3.028600E-02\n",
      "Epoch 4 -- Loss 2.902006E-02\n",
      "Epoch 5 -- Loss 2.751315E-02\n",
      "Epoch 6 -- Loss 2.609730E-02\n",
      "Epoch 7 -- Loss 2.527577E-02\n",
      "Epoch 8 -- Loss 2.445731E-02\n",
      "Epoch 9 -- Loss 2.390423E-02\n",
      "Epoch 10 -- Loss 2.326568E-02\n",
      "Epoch 11 -- Loss 2.277314E-02\n",
      "Epoch 12 -- Loss 2.234956E-02\n",
      "Epoch 13 -- Loss 2.192946E-02\n",
      "Epoch 14 -- Loss 2.162480E-02\n",
      "Epoch 15 -- Loss 2.128638E-02\n",
      "Epoch 16 -- Loss 2.097728E-02\n",
      "Epoch 17 -- Loss 2.063996E-02\n",
      "Epoch 18 -- Loss 2.040741E-02\n",
      "Epoch 19 -- Loss 2.009798E-02\n",
      "Epoch 20 -- Loss 1.970065E-02\n",
      "Epoch 21 -- Loss 1.940156E-02\n",
      "Epoch 22 -- Loss 1.908588E-02\n",
      "Epoch 23 -- Loss 1.880674E-02\n",
      "Epoch 24 -- Loss 1.860345E-02\n",
      "Epoch 25 -- Loss 1.833004E-02\n",
      "Epoch 26 -- Loss 1.808931E-02\n",
      "Epoch 27 -- Loss 1.783240E-02\n",
      "Epoch 28 -- Loss 1.761018E-02\n",
      "Epoch 29 -- Loss 1.738038E-02\n",
      "Epoch 30 -- Loss 1.716311E-02\n",
      "Epoch 31 -- Loss 1.697613E-02\n",
      "Epoch 32 -- Loss 1.677983E-02\n",
      "Epoch 33 -- Loss 1.659287E-02\n",
      "Epoch 34 -- Loss 1.644216E-02\n",
      "Epoch 35 -- Loss 1.627739E-02\n",
      "Epoch 36 -- Loss 1.608818E-02\n",
      "Epoch 37 -- Loss 1.592565E-02\n",
      "Epoch 38 -- Loss 1.577042E-02\n",
      "Epoch 39 -- Loss 1.561033E-02\n",
      "Epoch 40 -- Loss 1.549894E-02\n",
      "Epoch 41 -- Loss 1.541354E-02\n",
      "Epoch 42 -- Loss 1.526777E-02\n",
      "Epoch 43 -- Loss 1.512599E-02\n",
      "Epoch 44 -- Loss 1.502578E-02\n",
      "Epoch 45 -- Loss 1.491582E-02\n",
      "Epoch 46 -- Loss 1.478723E-02\n",
      "Epoch 47 -- Loss 1.468290E-02\n",
      "Epoch 48 -- Loss 1.458533E-02\n",
      "Epoch 49 -- Loss 1.449740E-02\n",
      "Epoch 50 -- Loss 1.437772E-02\n",
      "Epoch 51 -- Loss 1.429485E-02\n",
      "Epoch 52 -- Loss 1.418855E-02\n",
      "Epoch 53 -- Loss 1.410023E-02\n",
      "Epoch 54 -- Loss 1.398374E-02\n",
      "Epoch 55 -- Loss 1.388290E-02\n",
      "Epoch 56 -- Loss 1.378736E-02\n",
      "Epoch 57 -- Loss 1.368166E-02\n",
      "Epoch 58 -- Loss 1.358496E-02\n",
      "Epoch 59 -- Loss 1.349654E-02\n",
      "Epoch 60 -- Loss 1.340339E-02\n",
      "Epoch 61 -- Loss 1.332150E-02\n",
      "Epoch 62 -- Loss 1.324647E-02\n",
      "Epoch 63 -- Loss 1.316199E-02\n",
      "Epoch 64 -- Loss 1.306248E-02\n",
      "Epoch 65 -- Loss 1.295649E-02\n",
      "Epoch 66 -- Loss 1.286675E-02\n",
      "Epoch 67 -- Loss 1.277171E-02\n",
      "Epoch 68 -- Loss 1.268770E-02\n",
      "Epoch 69 -- Loss 1.259972E-02\n",
      "Epoch 70 -- Loss 1.253255E-02\n",
      "Epoch 71 -- Loss 1.246408E-02\n",
      "Epoch 72 -- Loss 1.237900E-02\n",
      "Epoch 73 -- Loss 1.230979E-02\n",
      "Epoch 74 -- Loss 1.223751E-02\n",
      "Epoch 75 -- Loss 1.217652E-02\n",
      "Epoch 76 -- Loss 1.209728E-02\n",
      "Epoch 77 -- Loss 1.203455E-02\n",
      "Epoch 78 -- Loss 1.198258E-02\n",
      "Epoch 79 -- Loss 1.191357E-02\n",
      "Epoch 80 -- Loss 1.186136E-02\n",
      "Epoch 81 -- Loss 1.179154E-02\n",
      "Epoch 82 -- Loss 1.174032E-02\n",
      "Epoch 83 -- Loss 1.167088E-02\n",
      "Epoch 84 -- Loss 1.163910E-02\n",
      "Epoch 85 -- Loss 1.157495E-02\n",
      "Epoch 86 -- Loss 1.152897E-02\n",
      "Epoch 87 -- Loss 1.148239E-02\n",
      "Epoch 88 -- Loss 1.139560E-02\n",
      "Epoch 89 -- Loss 1.135580E-02\n",
      "Epoch 90 -- Loss 1.131723E-02\n",
      "Epoch 91 -- Loss 1.125041E-02\n",
      "Epoch 92 -- Loss 1.122084E-02\n",
      "Epoch 93 -- Loss 1.114822E-02\n",
      "Epoch 94 -- Loss 1.110824E-02\n",
      "Epoch 95 -- Loss 1.105959E-02\n",
      "Epoch 96 -- Loss 1.098893E-02\n",
      "Epoch 97 -- Loss 1.097371E-02\n",
      "Epoch 98 -- Loss 1.090757E-02\n",
      "Epoch 99 -- Loss 1.086679E-02\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    loss = train(criterion, model, loader, optimizer)\n",
    "    print(\"Epoch {} -- Loss {:3E}\".format(epoch, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('../models/lstm_mlp_lstm'.format(n_epochs-1))\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187425\n",
      "torch.Size([187425, 1, 80])\n",
      "187425\n",
      "torch.Size([187425, 1, 80])\n"
     ]
    }
   ],
   "source": [
    "val_batch_size = 128\n",
    "sequence_length = 80\n",
    "val_dataset=DIIRDataSet(train_input.squeeze(0), train_target.squeeze(0), sequence_length)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle = False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_file(path):\n",
    "    print(\"-\" * 10)\n",
    "    print(\"Source:\", path)\n",
    "    print(\"-\" * 10)\n",
    "    print(f\" - File size: {os.path.getsize(path)} bytes\")\n",
    "    print(f\" - {torchaudio.info(path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_audio(batch):\n",
    "    #1024,512,1\n",
    "    out_batch = batch.detach().cpu()\n",
    "    out_batch = out_batch.squeeze(-1).flatten()\n",
    "    print(out_batch.shape)\n",
    "    return out_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14994001, 80])\n",
      "torch.Size([1199520080])\n",
      "torch.Size([1199520080])\n",
      "Exporting ../output/lstm_mlp_lstm_h1.wav\n",
      "----------\n",
      "Source: ../output/lstm_mlp_lstm_h1.wav\n",
      "----------\n",
      " - File size: 3598560284 bytes\n",
      " - AudioMetaData(sample_rate=44100, num_frames=1199520080, num_channels=1, bits_per_sample=24, encoding=PCM_S)\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "out_path = '../output/'\n",
    "sample_rate = 44100\n",
    "save_tensor = torch.zeros(14994001,80)\n",
    "with torch.no_grad():\n",
    "    for i, val_batch in enumerate(val_loader):\n",
    "        input_seq_batch = val_batch['input'].to(device)\n",
    "        #target_seq_batch = val_batch['target'].to(device)\n",
    "        predicted_output = model(input_seq_batch)\n",
    "        output_tmp = predicted_output.squeeze().detach().cpu()\n",
    "        #print(output_tmp.shape)\n",
    "        save_tensor[i,:] = output_tmp\n",
    "    \n",
    "    print(save_tensor.shape)\n",
    "    out_audio = save_audio(save_tensor)\n",
    "    print(out_audio.shape)\n",
    "    path = os.path.join(out_path, \"lstm_mlp_lstm_h1.wav\")\n",
    "    print(\"Exporting {}\".format(path))\n",
    "    sf.write(path, out_audio, sample_rate,'PCM_24')\n",
    "    #torchaudio.save(path, out_audio, sample_rate, encoding=\"PCM_S\", bits_per_sample=16)\n",
    "    inspect_file(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4])\n",
      "torch.Size([3, 3, 7])\n"
     ]
    }
   ],
   "source": [
    "#small test on padding\n",
    "t4d = torch.ones(3, 3, 4)\n",
    "print(t4d.shape)\n",
    "out = F.pad(t4d, (3,0)) #\"constant\", 0\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4d[1,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1,1,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
